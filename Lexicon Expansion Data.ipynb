{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c24099f",
   "metadata": {},
   "source": [
    "# Lexicon Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28635e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from nltk.book import *\n",
    "from string import punctuation\n",
    "from collections import Counter,defaultdict\n",
    "import os\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ddb3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the vape file from moodle make sure to encode utf-8\n",
    "\n",
    "vape_tweets = []\n",
    "\n",
    "with open(\"vaping_tweets.csv\", encoding = \"UTF-8\") as input_file:\n",
    "    next(input_file)\n",
    "    vape_tweets = [tweet.strip() for tweet in input_file.readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b9139a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vape_tweets[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e457fc",
   "metadata": {},
   "source": [
    "# Building the code step by step for lexicon expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea6bbcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ok I have the file read in now let's pick three words\n",
    "#hooked too many\n",
    "#Keep highschool\n",
    "#Keep ripping\n",
    "#Keep locker\n",
    "\n",
    "count = 0\n",
    "\n",
    "for tweet in vape_tweets :\n",
    "    if 'locker' in tweet:\n",
    "        #print(tweet)\n",
    "        count += 1\n",
    "#print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b73299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset first create key word tweet list and then a non keyword tweet list\n",
    "\n",
    "keywrd_tweets = []\n",
    "non_keywrd_tweets = []\n",
    "\n",
    "for tweet in vape_tweets :\n",
    "    if 'locker' in tweet or 'ripping' in tweet or 'highschool' in tweet:\n",
    "           keywrd_tweets.append(tweet)\n",
    "    else:\n",
    "        non_keywrd_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a195e7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9445"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's find words with a high concentration within our keywrd_tweets list\n",
    "\n",
    "# first let's change our list of tweets to one long string to tokenize / normalize\n",
    "\n",
    "keywrd_twts = \" \".join(keywrd_tweets)\n",
    "\n",
    "#now lets tokenize/normalize and remove sw's\n",
    "\n",
    "keywrd_twts = [w.lower() for w in keywrd_twts.split() if w.lower() not in sw and w.isalpha()]\n",
    "#print(keywrd_twts)\n",
    "\n",
    "#now let's get the overall length of keywrd_tweet\n",
    "len(keywrd_twts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "748f3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's create our concentration dictionary\n",
    "concentration_1 = dict()\n",
    "#first create a freq distribution to get the count of each word in keywrd_tweets\n",
    "\n",
    "fd1 = nltk.FreqDist(keywrd_twts)\n",
    "fd1\n",
    "\n",
    "#now lets get our concentrations\n",
    "\n",
    "for word,count in fd1.items():\n",
    "    concentration_1[word]= count/len(keywrd_twts)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23679274",
   "metadata": {},
   "source": [
    "Now that I have figured out the steps above, I will combine the steps into one large function. I will leave the code below as a jumping off point to test words before throwing them into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d0385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test word counts\n",
    "#bathroom\n",
    "#teacher\n",
    "#principal \n",
    "\n",
    "word_set= []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for tweet in vape_tweets :\n",
    "    if 'principal' in tweet:\n",
    "        #print(tweet)\n",
    "        count += 1\n",
    "#print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db736a",
   "metadata": {},
   "source": [
    "# Building the Concentration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e5ff997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets repeat steps 3-5 and add some words to our list\n",
    "\n",
    "def get_datasets(word1,word2,word3) :\n",
    "    \n",
    "    keywrd_tweets = []\n",
    "    non_keywrd_tweets = []\n",
    "\n",
    "    for tweet in vape_tweets :\n",
    "        if (word1) in tweet or (word2) in tweet or (word3) in tweet:\n",
    "               keywrd_tweets.append(tweet)\n",
    "        else:\n",
    "            non_keywrd_tweets.append(tweet)\n",
    "    \n",
    "    #now let's find words with a high concentration within our keywrd_tweets list\n",
    "\n",
    "    # first let's change our list of tweets to one long string to tokenize / normalize\n",
    "\n",
    "    keywrd_twts = \" \".join(keywrd_tweets)\n",
    "\n",
    "    #now lets tokenize/normalize and remove sw's\n",
    "\n",
    "    keywrd_twts = [w.lower() for w in keywrd_twts.split() if w.lower() not in sw and w.isalpha()]\n",
    "    #print(keywrd_twts)\n",
    "\n",
    "    #now let's get the overall length of keywrd_tweet\n",
    "    print(len(keywrd_twts))\n",
    "    \n",
    "    ## now let's create our concentration dictionary\n",
    "    concentration_1 = dict()\n",
    "    #first create a freq distribution to get the count of each word in keywrd_tweets\n",
    "\n",
    "    fd1 = nltk.FreqDist(keywrd_twts)\n",
    "    \n",
    "\n",
    "    #now lets get our concentrations\n",
    "\n",
    "    for word,count in fd1.items():\n",
    "        concentration_1[word]= count/len(keywrd_twts)\n",
    "        \n",
    "    #sort the dictionary and pick words\n",
    "    concentration_1 = (sorted(concentration_1.items(), key=lambda item: -1*item[1]))\n",
    "\n",
    "    return(concentration_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd893ee",
   "metadata": {},
   "source": [
    "# Expanding the wordset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14517ac8",
   "metadata": {},
   "source": [
    "Alright now that I have built my function, it's time to expand my word list in the cells below, I'll repeat the process three times growing my word list to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a8de0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first word_set\n",
    "#get_datasets('highschool','ripping','locker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "569d0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd word_set\n",
    "#get_datasets('pen','kid','highschoolers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ecb031c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3rd word_set\n",
    "#get_datasets('school','nic','hit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94788a",
   "metadata": {},
   "source": [
    "# Final Function Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e9d23",
   "metadata": {},
   "source": [
    "Now that we have expanded our wordlist to nine words based on their concentrations. I will write a final function to compare the target tweets data set and the non target tweets dataset. I'll also pull in my get patterns function to get the breakdowns of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7cb5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below quickly turns my two data sets into large strings for get patterns function to work correctly.\n",
    "\n",
    "keywrd_tweets = []\n",
    "non_keywrd_tweets = []\n",
    "\n",
    "for tweet in vape_tweets:\n",
    "    if 'school' in tweet or 'nic' in tweet or 'hit' in tweet or 'highschoolers' in tweet or 'kid' in tweet or 'pen' in tweet or 'locker' in tweet or 'ripping' in tweet or 'highschool' in tweet:\n",
    "        keywrd_tweets.append(tweet)\n",
    "    else:\n",
    "        non_keywrd_tweets.append(tweet)\n",
    "        \n",
    "\n",
    "keywrd_tweets = ' '.join(keywrd_tweets)\n",
    "non_keywrd_tweets = ' '.join(non_keywrd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c4b79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text)  :\n",
    "    \"\"\"\n",
    "        This function takes text as an input and returns a dictionary of statistics,\n",
    "        after cleaning the text. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll make things a big clearer by initializing the \n",
    "    # statistics here. These are placeholder values.\n",
    "    total_tokens = 0\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_10 = Counter()\n",
    "    \n",
    "    # Do your tokenization and normalization here but in this case I think we will only want to\n",
    "    \n",
    "    text = [w.lower() for w in text.split() if w.lower() not in sw and w.isalpha()]\n",
    "    #text = [w.lower() for w in text if w.lower() not in sw]\n",
    "    \n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    total_tokens = len(text)\n",
    "    unique_tokens = len(set(text))\n",
    "    avg_token_len = sum(len(w) for w in text) / len(text)\n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "    text_count= Counter(text)\n",
    "    top_10 = text_count.most_common(10)\n",
    "    \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_10':top_10}\n",
    "\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c21b6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare the two datasets using our dictionary build from group comparison\n",
    "\n",
    "def final_comparison(word1,word2,word3,word4,word5,word6,word7,word8,word9,num_words=10,ratio_cutoff=5):\n",
    "    \n",
    "    keywrd_tweets = []\n",
    "    non_keywrd_tweets = []\n",
    "\n",
    "    for tweet in vape_tweets:\n",
    "        if word1 in tweet or word2 in tweet or word3 in tweet or word4 in tweet or word5 in tweet or word6 in tweet or word7 in tweet or word8 in tweet or word9 in tweet:\n",
    "            keywrd_tweets.append(tweet)\n",
    "        else:\n",
    "            non_keywrd_tweets.append(tweet)\n",
    "                \n",
    "    \n",
    "    #now let's find words with a high concentration within our keywrd_tweets list\n",
    "\n",
    "    # first let's change our list of tweets to one long string to tokenize / normalize\n",
    "\n",
    "    keywrd_twts = \" \".join(keywrd_tweets)\n",
    "    non_keywrd_twts = \" \".join(non_keywrd_tweets)\n",
    "\n",
    "    #now lets tokenize/normalize and remove sw's\n",
    "\n",
    "    keywrd_twts = [w.lower() for w in keywrd_twts.split() if w.lower() not in sw and w.isalpha()]\n",
    "    non_keywrd_twts = [w.lower() for w in non_keywrd_twts.split() if w.lower() not in sw and w.isalpha()]\n",
    "    \n",
    "    ## now let's create our concentration dictionary FOR BOTH LISTS\n",
    "    concentration_1 = dict()\n",
    "    #first create a freq distribution to get the count of each word in keywrd_tweets\n",
    "\n",
    "    fd1 = nltk.FreqDist(keywrd_twts)\n",
    "    \n",
    "    concentration_2 = dict()\n",
    "\n",
    "    fd2 = nltk.FreqDist(non_keywrd_twts)\n",
    "\n",
    "    #now lets get our concentrations for both sets of tweets\n",
    "\n",
    "    for word,count in fd1.items():\n",
    "        concentration_1[word]= count/len(keywrd_twts)\n",
    "    \n",
    "    for word,count in fd2.items():\n",
    "        concentration_2[word]= count/len(non_keywrd_twts)\n",
    "\n",
    "\n",
    "    #now let's bring in our comparison dictionary from group comparison and get our stats\n",
    "    keywrd_twts_vs_non_keywrd_twts = defaultdict()\n",
    "\n",
    "    for word, concentration1 in concentration_1.items() :\n",
    "\n",
    "        if fd1[word]> ratio_cutoff and fd2[word] > ratio_cutoff:\n",
    "\n",
    "            concentration2 = concentration_2[word]\n",
    "\n",
    "            keywrd_twts_vs_non_keywrd_twts[word]= concentration1/concentration2\n",
    "    \n",
    "    \n",
    "    non_keywrd_twts_vs_keywrd_twts = defaultdict()\n",
    "\n",
    "    for word, concentration2 in concentration_2.items() :\n",
    "\n",
    "        if fd2[word]> ratio_cutoff and fd1[word] > ratio_cutoff:\n",
    "\n",
    "            concentration1 = concentration_1[word]\n",
    "\n",
    "            non_keywrd_twts_vs_keywrd_twts[word]= concentration2/concentration1\n",
    "    \n",
    "    \n",
    "    results=dict()\n",
    "    \n",
    "    results[\"keywrd_twts_vs_non_keywrd_twts\"] = sorted(keywrd_twts_vs_non_keywrd_twts.items(),  key=lambda item: -1*item[1])[:num_words]\n",
    "    results[\"non_keywrd_twts_vs_keywrd_twts\"] = sorted(non_keywrd_twts_vs_keywrd_twts.items(),  key=lambda item: -1*item[1])[:num_words]\n",
    "    \n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24477db9",
   "metadata": {},
   "source": [
    "# Final Results section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3194f2",
   "metadata": {},
   "source": [
    "In this section both functions can be used to compare different data sets, and finish out lexicon expansion. Enjoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "303c7cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keywrd_twts_vs_non_keywrd_twts': [('divided', 193.12493659558882),\n",
       "  ('districts', 63.263927452792856),\n",
       "  ('bryson', 59.62550908989748),\n",
       "  ('erin', 57.277244177110425),\n",
       "  ('appropriate', 35.4159188238076),\n",
       "  ('steven', 32.67151183008082),\n",
       "  ('contains', 30.586874321519687),\n",
       "  ('charli', 30.025119371844273),\n",
       "  ('dab', 24.432608846843046),\n",
       "  ('leaked', 21.255930018022813)],\n",
       " 'non_keywrd_twts_vs_keywrd_twts': [('ipad', 84.63030466359538),\n",
       "  ('premarital', 71.5345794004867),\n",
       "  ('extramarital', 71.18477705623006),\n",
       "  ('spotted', 53.51975867126879),\n",
       "  ('publishes', 47.48566823284143),\n",
       "  ('grabs', 44.73433825589963),\n",
       "  ('tiwa', 41.90473083129201),\n",
       "  ('motivating', 41.01432486409324),\n",
       "  ('subohm', 35.5284823306066),\n",
       "  ('storage', 34.38265542089409)]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_comparison('school','nic','hit','pen','kid','highschoolers','highschool','ripping','locker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e930f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 1437860,\n",
       " 'unique_tokens': 40350,\n",
       " 'avg_token_length': 5.563310058002865,\n",
       " 'lexical_diversity': 0.028062537381942608,\n",
       " 'top_10': [('juul', 59587),\n",
       "  ('nicotine', 45649),\n",
       "  ('vape', 18810),\n",
       "  ('smoking', 17023),\n",
       "  ('vaping', 15302),\n",
       "  ('like', 14951),\n",
       "  ('kids', 10774),\n",
       "  ('get', 10296),\n",
       "  ('cigarettes', 8642),\n",
       "  ('one', 7620)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(keywrd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25645b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 2348853,\n",
       " 'unique_tokens': 59326,\n",
       " 'avg_token_length': 5.4727400991036905,\n",
       " 'lexical_diversity': 0.025257434160417872,\n",
       " 'top_10': [('juul', 67947),\n",
       "  ('smoking', 52100),\n",
       "  ('vaping', 41436),\n",
       "  ('vape', 36903),\n",
       "  ('new', 25739),\n",
       "  ('like', 19682),\n",
       "  ('cigarettes', 17782),\n",
       "  ('logic', 14770),\n",
       "  ('aspire', 12420),\n",
       "  ('get', 12357)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(non_keywrd_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
